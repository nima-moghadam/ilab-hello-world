{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ih7e5O6rX_",
        "tags": []
      },
      "source": [
        "# Running InstructLab with a GPU\n",
        "\n",
        "<ul>\n",
        "<li>Contributors: InstructLab team and IBM Research Technology Education team:\n",
        "<li>Questions and support: kochel@us.ibm.com, IBM.Research.JupyterLab@ibm.com\n",
        "<li>Version: 1.0.14\n",
        "<li>Release date: 2025-03-20\n",
        "<li>Compute requirements: Colab with GPU\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ics9GgZ-6rYB",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Summary\n",
        "This Jupyter notebook demonstrates InstructLab, an open source AI project that facilitates knowledge and skills contributions to Large Language Models (LLMs). InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081). The open source InstructLab repository is available [here](https://github.com/instructlab/instructlab) and provides additional documentation on using InstructLab.\n",
        "\n",
        "The InstructLab method consists of three major components:\n",
        "* **Taxonomy-driven data curation:**  The taxonomy is a set of training data curated by humans as examples of new knowledge and skills for the model.\n",
        "* **Large-scale synthetic data generation:** A teacher model is used to generate new examples based on the seed training data. Since synthetic data can vary in quality, InstructLab adds an automated step to refine the example answers, ensuring they are grounded and safe.\n",
        "* **Iterative model alignment tuning:** The model is retrained based on the synthetic data. The InstructLab method includes two tuning phases: knowledge tuning, followed by skill tuning.\n",
        "\n",
        "<img src=\"https://github.com/KenOcheltree/ilab-colab/blob/main/data/images/Flow.png?raw=1\" width=\"800\">\n",
        "\n",
        "InstructLab can take the form of an open source installation or a Red Hat AI InstructLab installation. In this notebook, we will demonstrate the open source version of InstructLab running on Colab with a GPU, broken into the following major sequential sections:\n",
        "* Configuring InstructLab\n",
        "* Generating Syntehtic Data\n",
        "* Training with InstructLab\n",
        "* Inferencing with InstructLab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJjDTcrmAPVS"
      },
      "source": [
        "# Running this Notebook\n",
        "\n",
        "**IMPORTANT:** This notebook must be run within a Colab GPU runtime. You can check you are running with a GPU by selecting Runtime-> Change Runtime Type and confirming that a GPU Runtime is selected. While this notebook can be started on a free Colab account, the GPUs availabe with a free access do not have sufficient memory to run InstructLab training.\n",
        "\n",
        "You can run this notebook either:\n",
        "- Running All Cells by selecting Runtime->Run all\n",
        "- Cell by cell by selecting the arrow on each code cell and running them sequentially.\n",
        "\n",
        "Once the Configuring Instructlab section has been run, the other sections of this notebook can be repeatedly run on other data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlk9FjFB6rYD",
        "tags": []
      },
      "source": [
        "# Section 1. Configure InstructLab\n",
        "\n",
        "## Step 1.1 Environment Configuration\n",
        "Replicate the ilab data repository containing the pip requirements and data files and run the pip installs that require a reset.\n",
        "\n",
        "**IMPORTANT:** Run the next cell, allow it to complete running, then Restart the session , run the following cell to specify parameters and then you can run the remainder of the notebook. Ignore any spurious pip install warnings or errors.\n",
        "\n",
        "After selecting parameters, the remainder of this notebook can be run either:\n",
        "- Running All Cells by selecting Runtime->Run cell and below\n",
        "- Cell by cell by selecting the arrow on each code cell and running them sequentially.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rde52-02CfiL"
      },
      "outputs": [],
      "source": [
        "# Run this cell, then perform the requested Reset\n",
        "import os\n",
        "if not os.path.exists(\"ilab\"):\n",
        "    !git clone https://github.com/KenOcheltree/ilab.git\n",
        "!pip install numpy==1.26.4 torch==2.5.1 psutil==7.0.0 pillow==10.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dmB_IVBPkZ1",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Step 1.2 Optionally, provide your own InstructLab QNA data set\n",
        "\n",
        "You can optionally provide your own InstructLab QNA file for processing in this step.\n",
        "\n",
        "**Note:** You may want to run this notebook with an existing dataset before creating your own to understand the InstructLab flow.\n",
        "\n",
        "Follow these steps to add your own dataset:\n",
        "1. Create your own qna.yaml file following the directions on the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
        "1. Create a questions.txt file with related sample questions to use on inferencing.\n",
        "1. Add your qna.yaml and sample questions.txt files to the /content/ilab/data/your_content_1 folder or the /content/ilab/data/your_content_2 folder by dragging and dropping them in the desired folder.\n",
        "1. Double click on the /content/ilab/config.json file to edit and specify the qna_location where your data resides within the Dewey Decimal classification system. Close and save the config.json file.\n",
        "1. You can now specify to run with your own data by selecting **Your Content 1** or **Your Content 2** in the next code cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_SZDBZgMVa"
      },
      "source": [
        "## Step 1.3 Select InstructLab Parameters\n",
        "Run this next cell, select the following parameters, then follow the direction in the next text cell to run the notebook.\n",
        "\n",
        "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\" and \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E0Z6oO2L_3I"
      },
      "outputs": [],
      "source": [
        "# Run this second cell to show parameters\n",
        "import ipywidgets as widgets\n",
        "#See instructions on placing your hf_token in colab userdata\n",
        "from google.colab import userdata\n",
        "# hf_token=userdata.get('hf_token')\n",
        "hf_token=\"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\"\n",
        "data_set = widgets.ToggleButtons(\n",
        "    options=['2024 Oscars', 'Quantum', 'Agentic AI', 'Your Content 1', 'Your Content 2'],\n",
        "    description='Dataset:', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "sdg_pipe = widgets.ToggleButtons(\n",
        "    options=['Simple', 'Full with GPU'], description='Processing:', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "instr=widgets.ToggleButtons(\n",
        "    options=['Default (>450)','>15', '>50', '>200', '>500', '>1000'],\n",
        "    description='# of QNAs:', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "train_pipe = widgets.ToggleButtons(\n",
        "    options=['Simple with GPU','Accelerated GPU'],description='Processing',style={\"button_width\":\"auto\"}\n",
        ")\n",
        "epoch=widgets.ToggleButtons(\n",
        "    options=['1', '2', '3', '4', '5', '10', '15'],description='Epochs:',style={\"button_width\":\"auto\"}\n",
        ")\n",
        "it=widgets.ToggleButtons(\n",
        "    options=['1', '3', '5','10','20','50','100','200'],description='Iterations:',style={\"button_width\":\"auto\"}\n",
        ")\n",
        "questions=widgets.ToggleButtons(options=['Yes','No'],description='Live Q&A:',style={\"button_width\":\"auto\"})\n",
        "download=widgets.ToggleButtons(options=['Yes','No'],description='Download:',style={\"button_width\":\"auto\"}\n",
        ")\n",
        "print(\"\\nSelect the Dataset for this run:\")\n",
        "display(data_set)\n",
        "print(\"Select the Synthetic Data Generation parameter to use:\")\n",
        "sdg_pipe.value='Simple'\n",
        "display(sdg_pipe)\n",
        "instr.value = 'Default (>450)'\n",
        "display(instr)\n",
        "print(\"Select the Training parameters to use:\")\n",
        "train_pipe.value='Simple with GPU'\n",
        "#display(train_pipe)\n",
        "epoch.value=\"3\"\n",
        "display(epoch)\n",
        "it.value=\"5\"\n",
        "display(it)\n",
        "print(\"Select what to do with the model after training:\")\n",
        "questions.value=\"Yes\"\n",
        "display(questions)\n",
        "download.value=\"No\"\n",
        "display(download)\n",
        "print(\"After selecting the parameters, select the next cell and then choose Runtime->Run cell and below\")\n",
        "print(\"When that run completes, you can come here, choose different parameters and rerun at the next cell with Runtime->Run cell and below\")\n",
        "print(\"Note: You can also go back and rerun individual sections of the notebook with different parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVAfX3Q5CfiL",
        "tags": []
      },
      "source": [
        "## 1.4 Complete Environment Set Up and Optionally Run All\n",
        "This next code cell installs the remainder of the reuired pip packages and takes about 7 minutes to run.\n",
        "\n",
        "If you perform **Runtime->Run cell and below** on this cell, the rest of notebook will take about an hour to run. After running, it will present a prompt for providing questions to the pre-trained and trained models to test improvements in the model.\n",
        "\n",
        "**Note:** Please ignore the pip dependency errors that appear in the output of the pip installs. They do not affect the successful running of Instructlab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFtTdSAbCfiL"
      },
      "outputs": [],
      "source": [
        "# Run the rest of the notebook by selecting this third cell and choosing \"Runtime->Run cell and below\"\n",
        "!pip cache remove llama_cpp_python\n",
        "!pip install -r ilab/requirements_gpu.txt\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mlth0Vetawj"
      },
      "source": [
        "Wrap code cell output for ease of reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abrPE0P18BWC"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_TTU5vUhXvM",
        "tags": []
      },
      "source": [
        "## Step 1.5 Check Starting Configuration\n",
        "### Check InstructLab Version\n",
        "\n",
        "Check that InstructLab is installed properly and is configured for using a GPU.\n",
        "\n",
        "The first line from 'InstructLab' section will give the InstructLab version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqjqIGE06rYE"
      },
      "outputs": [],
      "source": [
        "!ilab system info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5EcJoSS6rYD",
        "tags": []
      },
      "source": [
        "<a id=\"IL1_check\"></a>\n",
        "## Perform Imports and Check for a GPU\n",
        "\n",
        "This code cell checks for a GPU in the configuration. This notebook requires a GPU in the configuration to run properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLDdSBiX6rYD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from IPython.display import Image, display\n",
        "from datasets import load_dataset\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import json\n",
        "import subprocess\n",
        "import shutil\n",
        "import ruamel.yaml\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '64'\n",
        "Norm = \"<p style='font-family:IBM Plex Sans;font-size:20px'>\"\n",
        "\n",
        "notebook_dir='/content/ilab/'\n",
        "os.chdir(notebook_dir)\n",
        "\n",
        "## torch and cuda version check\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "if torch.cuda.is_available() is False:\n",
        "    print(\"No GPU in configuration\")\n",
        "else:\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    print(\"GPU(s) are Available\")\n",
        "    gpus=torch.cuda.device_count()\n",
        "    if gpus==1:\n",
        "      gpu_type=torch.cuda.get_device_name(0)\n",
        "      print(\"One GPU of Type: \", gpu_type)\n",
        "    else:\n",
        "      print(\"ERROR: More than 1 GPU in configuration: \",gpus)\n",
        "print(\"Starting directory: \"+ os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv5uMvQ-a-ZF",
        "tags": []
      },
      "source": [
        "<a id=\"IL1_config\"></a>\n",
        "## Step 1.6 Configure InstructLab\n",
        "\n",
        "### Create InstructLab config file\n",
        "The InstructLab configuration is captured in the *config.yaml* file. This step creates the config.yaml file and sets:\n",
        "- **taxomony_path = taxonomy** - the root location of the taxonomy is set to the taxonomy folder in instructlab-latest\n",
        "- **model_path = models/merlinite-7b-lab-Q4_K_M.gguf** - the default model is set to merlinite\n",
        "\n",
        "**Note:** The default directories for InstructLab are the following. If you initialize InstructLab on your own system, it will default to the following:\n",
        "* **Downloaded Models:**  ~/.cache/instructlab/models/ - Contains all downloaded large language models, including the saved output of ones you generate with ilab.\n",
        "* **Synthetic Data:** ~/.local/share/instructlab/datasets/ - Contains data output from the SDG phase, built on modifications to the taxonomy repository.\n",
        "* **Taxonomy:** ~/.local/share/instructlab/taxonomy/ - Contains the skill and knowledge data.\n",
        "* **Training Output:** ~/.local/share/instructlab/checkpoints/ - Contains the output of the training process.\n",
        "* **config.yaml:** ~/.config/instructlab/config.yaml - Contains the config.yaml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX9s4XZx6rYF"
      },
      "outputs": [],
      "source": [
        "#Remove Colab Sample directory\n",
        "if os.path.exists(\"sample_data\"):\n",
        "    print(\"removing sample_data\")\n",
        "    shutil.rmtree(\"sample_data\")\n",
        "    os.chdir(\"ilab\")\n",
        "\n",
        "#Initialize ilab\n",
        "base_dir=\"/root/\"\n",
        "##Choose the base model as granite or mixtral\n",
        "model_dir=\"models\"\n",
        "model_name=\"granite-7b-lab-Q4_K_M.gguf\"\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "\n",
        "taxonomy_path='taxonomy'\n",
        "\n",
        "## Define the file name\n",
        "file_name = \"config.yaml\"\n",
        "if os.path.exists(file_name):\n",
        "    os.remove(file_name)\n",
        "    print(f\"ilab was already initialized. {file_name} has been deleted. Reinitialized\")\n",
        "else:\n",
        "    print(f\"ilab was not initialized yet. {file_name} does not exist.\")\n",
        "\n",
        "##Remove old data\n",
        "if os.path.exists(\"taxonomy\"):\n",
        "    print(\"removing taxonomy\")\n",
        "    shutil.rmtree(\"taxonomy\")\n",
        "if os.path.exists(base_dir+\".cache/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".cache/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".cache/instructlab\")\n",
        "if os.path.exists(base_dir+\".config/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".config/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".config/instructlab\")\n",
        "if os.path.exists(base_dir+\".local/share/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".local/share/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".local/share/instructlab\")\n",
        "\n",
        "print(f\"ilab model is {model_path}.\")\n",
        "print('#############################################################')\n",
        "print(' ')\n",
        "\n",
        "command = f\"\"\"\n",
        "ilab config init<<EOF\n",
        "{taxonomy_path}\n",
        "Y\n",
        "{model_path}\n",
        "0\n",
        "EOF\n",
        "\"\"\"\n",
        "\n",
        "## Using the ! operator to run the command\n",
        "!echo \"Running ilab config init\"\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8rbeRKE6rYF"
      },
      "source": [
        "### Display the config.yaml file\n",
        "We examine the base configuration for identifying parameters for changing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZflN-eeu6rYF",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "##to copy config.yaml to local directory\n",
        "!cp /root/.config/instructlab/config.yaml .\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAL743VA6rYH"
      },
      "source": [
        "### Customize LLM Models and copy to notebook for use\n",
        "\n",
        "This cell changes the models to use for the generate stage. The mistral model as the teacher model in the generate step and as the student model to be trained.\n",
        "\n",
        "If you want to customize other models for generation or the training phase, you would specify the models in this step.\n",
        "\n",
        "This step specifies that the models to be used will be from this notebook's models directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QILAQZYY6rYH",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "##Use ruamel.yaml to load the yaml file to preserve comments\n",
        "yaml = ruamel.yaml.YAML()\n",
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.load(file)\n",
        "\n",
        "##Upate to use the same models and just change the directory\n",
        "teacher_model_path = \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "base_model_path = \"models/instructlab/granite-7b-lab\"\n",
        "##judge_model_path = \"models/prometheus-eval/prometheus-8x7b-v2.0\"\n",
        "\n",
        "##config['evaluate']['mt_bench']['judge_model'] = judge_model_path\n",
        "##config['evaluate']['mt_bench_branch']['judge_model'] = judge_model_path\n",
        "config['generate']['model'] = teacher_model_path\n",
        "config['generate']['teacher']['model_path']= teacher_model_path\n",
        "##config['train']['phased_mt_bench_judge']=judge_model_path\n",
        "\n",
        "#Update GPU information\n",
        "config['evaluate']['gpus']=gpus\n",
        "config['generate']['teacher']['vllm']['gpus']=gpus\n",
        "config['serve']['vllm']['gpus']=gpus\n",
        "config['train']['nproc_per_node']=gpus\n",
        "config['metadata']['gpu_count']=gpus\n",
        "if gpus==1:\n",
        "  config['train']['device']=\"cuda\"\n",
        "  if gpu_type[:6]==\"NVIDIA\":\n",
        "    config['metadata']['gpu_manufacturer']=\"Nvidia\"\n",
        "    config['metadata']['gpu_family']=gpu_type[7:]\n",
        "\n",
        "## Save the updated config.yaml file\n",
        "yaml.default_flow_style=False\n",
        "with open('config.yaml', 'w') as file:\n",
        "    yaml.dump(config, file)\n",
        "\n",
        "##copy the config file to the .config/instructlab/ where it is used by InstructLab\n",
        "!cp config.yaml {base_dir}.config/instructlab/\n",
        "\n",
        "print(\"Updated config.yaml successfully.\\n\")\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dZgO-FZ6rYH"
      },
      "source": [
        "<a id=\"IL1_down\"></a>\n",
        "## Step 1.7 Download Models\n",
        "The models that will be used in the InstructLab processing are downloaded in this step. Additional steps can be added if other models are used in processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI-25zVd6rYH"
      },
      "source": [
        "The merlinite model will be used as the teacher model for the simple pipeline in the **Training with InstructLab** section.\n",
        "\n",
        "The mistral-7b-instruct-v0.2.Q4_K_M model will be used as the teacher model for the full pipeline in that section.\n",
        "\n",
        "The granite07b-lab.gguf model is a quantized version of the granite-7b-lab model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCa2va8r6rYH"
      },
      "outputs": [],
      "source": [
        "models_dir=\"models\"\n",
        "!ilab model download --hf-token {hf_token} --model-dir {models_dir}\n",
        "\n",
        "# Download the granite 7b safe tensors model\n",
        "!ilab model download --repository instructlab/granite-7b-lab --hf-token {hf_token} --model-dir {models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDUmbbHLvxgg",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_0\"></a>\n",
        "# Section 2. Generating Synthetic Data\n",
        "\n",
        "\n",
        "This section demonstrates training with InstructLab. This section is part of a sequential notebook. Before running this section of the notebook, please ensure that you have run the Configuring InstructLab section of this notebook.\n",
        "\n",
        "In this section, we will demonstrate:\n",
        "- Creating a question and answer data file\n",
        "- Generating synthetic data for training\n",
        "- Training the LLM with the generated data\n",
        "\n",
        "The steps in this section are as follows:\n",
        "* Step 2.1 Specify the Data for this Run\n",
        "* Step 2.2 Create the Taxonomy Data Repository\n",
        "* Step 2.3 Generate Synthetic Data\n",
        "* Step 2.4 Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnOLAXRxvxgh",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_data\"></a>\n",
        "## Step 2.1 Specify the Data for this Run\n",
        "\n",
        "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\", \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltRTPBTVvxgh",
        "tags": []
      },
      "source": [
        "### Optionally, Create your own data set for InstructLab\n",
        "\n",
        "You can optionally provide your own InstructLab QNA file for processing in this step.\n",
        "\n",
        "Follow these steps to add your own dataset:\n",
        "1. Create your own qna.yaml file following the directions on the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
        "1. Create a questions.txt file with related sample questions to use on inferencing.\n",
        "1. Add your qna.yaml and sample questions.txt files to the /content/ilab/data/your_content_1 folder or the /content/ilab/data/your_content_2 folder by dragging and dropping them in the desired folder.\n",
        "1. Double click on the /content/ilab/config.json file to edit and specify the qna_location where your data resides within the Dewey Decimal classification system. Close and save the config.json file.\n",
        "1. You can now specify to run with your own data by selecting **Your Content 1** or **Your Content 2** in the next code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbnDz_4j5GNi"
      },
      "outputs": [],
      "source": [
        "print(\"\\nSelect the QNA dataset to add:\")\n",
        "display(data_set)\n",
        "print(\"After choosing your dataset, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olkDUaKfvxgh"
      },
      "outputs": [],
      "source": [
        "print(\"Step 2.2 Choose the Dataset for this Run\")\n",
        "if data_set.value=='2024 Oscars':\n",
        "    use_case=\"oscars\"\n",
        "elif data_set.value=='Quantum':\n",
        "    use_case=\"quantum\"\n",
        "elif data_set.value=='Agentic AI':\n",
        "    use_case=\"agentic_ai\"\n",
        "elif data_set.value=='Your Content 1':\n",
        "    use_case=\"your_content_1\"\n",
        "elif data_set.value=='Your Content 2':\n",
        "    use_case=\"your_content_2\"\n",
        "else:\n",
        "    use_case=\"undefined\"\n",
        "    print(\"ERROR: Undefined data set: \" + data_set.value + \" data\")\n",
        "\n",
        "with open('config.json', 'r') as f:\n",
        "    jsonData = json.load(f)\n",
        "\n",
        "if use_case!=\"undefined\":\n",
        "    qna_file=\"data/\" + use_case + \"/qna.yaml\"\n",
        "    qna_location=jsonData[\"use_cases\"][use_case][\"qna_location\"]\n",
        "    print(\"Using \" + data_set.value + \" data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBSnoObvxgh"
      },
      "source": [
        "<a id=\"IL2_taxonomy\"></a>\n",
        "## Step 2.2 Create the Taxonomy Data Repository\n",
        "Delete the prior repository, clone the empty taxonomy repository and place the QNA file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2s_UhRCvxgh"
      },
      "outputs": [],
      "source": [
        "#Delete the prior repository and clone the empty taxonomy repository\n",
        "print(\"Delete the prior repository and clone the empty taxonomy repository\")\n",
        "shell_command1 = f\"rm -rf taxonomy\"\n",
        "taxonomy_repo=jsonData[\"taxonomy_repo\"]\n",
        "shell_command2 = f\"git clone {taxonomy_repo}\"\n",
        "!{shell_command1}\n",
        "!{shell_command2}\n",
        "\n",
        "#show the QNA file\n",
        "print(\"Show the QNA file\")\n",
        "print_lines=40\n",
        "with open(qna_file, 'r') as input_file:\n",
        "    for line_number, line in enumerate(input_file):\n",
        "        if line_number > print_lines:  # line_number starts at 0.\n",
        "            break\n",
        "        print(line, end=\"\")\n",
        "\n",
        "# Place the QNA file in the proper taxonomy directory\n",
        "print(\"Place QNA file in taxononmy as: /taxonomy/\"+qna_location+\"/qna.yaml\")\n",
        "shell_command1 = f\"mkdir -p ./taxonomy/{qna_location}\"\n",
        "shell_command2 = f\"cp ./{qna_file} ./taxonomy/{qna_location}/qna.yaml\"\n",
        "!{shell_command1}\n",
        "!{shell_command2}\n",
        "\n",
        "print(\"Verify the taxonomy\")\n",
        "!ilab taxonomy diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKW-EIVvxgi",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_generate\"></a>\n",
        "## Step 2.3. Set data generation parameters\n",
        "\n",
        "#### Select pipeline\n",
        "\n",
        "InstructLab has three primary pipelines that can be used: simple, full and acellerated:\n",
        "- The **simple pipeline** runs fast and can be used for initial model and data testing.\n",
        "- The **full pipeline** runs all of the InstrctLab steps and takes more time but produces a better tuned model.\n",
        "\n",
        "**Note:** If you are running with a new or modifed dataset, you may want to use the **Simple pipeline** for the first run to verify the configuration\n",
        "\n",
        "#### Sepect number of samples to generate\n",
        "\n",
        "Data generation takes 19 minutes for generating 15 synthetic data samples. You may wish to generate a small number on your first run to verify the QNA dataset format.\n",
        "\n",
        "To produce **sufficient synthetic data** to focus training on the new material, **about 30 synthetic questions and answer pairs need to be generated** for each question and answer pair provided. This will require a proportionally longer time to generate, but will provide better training.\n",
        "\n",
        "Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, ilab data generate can start a server for you if you provide a fully qualified model path via --model.\n",
        "\n",
        "To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:\n",
        "\n",
        "    ilab data generate\n",
        "\n",
        "#### **Simple Pipeline**\n",
        "\n",
        "The Simple Pipeline works solely with Merlinite 7b Lab as the teacher model. The Simple Pipeline is called without GPU acceleration as follows:\n",
        "\n",
        "    ilab data generate --pipeline simple\n",
        "\n",
        "#### **Full Pipeline**\n",
        "\n",
        "The Full Pipeline runs the full processing with a GPU. Currently, the Full Pipeline only supports the Mixtral and Mistral Instruct Family models as the teacher model.  This is due to only supporting specific model prompt templates.\n",
        "\n",
        "Using a non-default model such as Mixtral-8x7B-Instruct-v0.1) to generate data with the Full Pipeline:\n",
        "\n",
        "    ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n",
        "\n",
        "**Note** Synthetic Data Generation can take from 2 minutes to 1+ hours to complete, depending on your computing resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rNYESyEvxgi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Select Pipeline to use\")\n",
        "display(sdg_pipe)\n",
        "display(instr)\n",
        "print(\"After making your selections for data generation, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so0dOw4vxgi",
        "tags": []
      },
      "source": [
        "### 2.4 Run data generation\n",
        "Data generation with a GPU can take 2 minutes or more to generate 15 synthetic data samples. It takes proportionately longer to generate more samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn6gODEcvxgi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "gen_directory = \"data/\"+ use_case+\"/ilab_generated/\"\n",
        "if instr.value == \"Default (>450)\":\n",
        "        sdg_factor=\"\"\n",
        "elif instr.value == '>15':\n",
        "    sdg_factor=\"--sdg-scale-factor 1\"\n",
        "elif instr.value == '>50':\n",
        "    sdg_factor=\"--sdg-scale-factor 3\"\n",
        "elif instr.value == '>200':\n",
        "    sdg_factor=\"--sdg-scale-factor 13\"\n",
        "elif instr.value == '>500':\n",
        "    sdg_factor=\"--sdg-scale-factor 33\"\n",
        "else:\n",
        "    sdg_factor=\"--sdg-scale-factor 67\"\n",
        "# 'Fast (Simple)', 'Full with CPU'\n",
        "if sdg_pipe.value == 'Simple':\n",
        "    pipeline = 'simple'\n",
        "    model = '--model models/instructlab/granite-7b-lab'\n",
        "    gpus = '--gpus 1'\n",
        "elif sdg_pipe.value == 'Full with GPU':\n",
        "    pipeline = 'full'\n",
        "    model = ''\n",
        "#   model = '--model models/instructlab/granite-7b-lab'\n",
        "    gpus = '--gpus 1'\n",
        "else:\n",
        "    print(\"ERROR: Undefined pipeline\")\n",
        "\n",
        "il_data_path= '/root/.local/share/instructlab/datasets/'\n",
        "#Remove old data so there is only one test_merlinite and train_merlinite after generation\n",
        "print(\"Remove old datasets\")\n",
        "!rm -rf {il_data_path}*\n",
        "#shell_command = f\"ilab --verbose data generate {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
        "shell_command = f\"ilab data generate {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
        "\n",
        "print(\"Generating data\")\n",
        "print(\"Running: !\"+shell_command)\n",
        "!{shell_command}\n",
        "\n",
        "#Rename results to  test_gen.jsonl and train_gen.jsonl and move to local data directory\n",
        "if not os.path.exists(gen_directory):\n",
        "    print(\"Create directory: \" + gen_directory)\n",
        "    !mkdir {gen_directory}\n",
        "file_cnt=0\n",
        "try:\n",
        "    for dirname in os.listdir(il_data_path):\n",
        "        date_path=il_data_path+'/'+ dirname + '/'\n",
        "        for filename in os.listdir(date_path):\n",
        "            if filename[:6]=='train_':\n",
        "                train_name= 'train_gen.jsonl'\n",
        "                print('Renaming '+ filename+ ' to ' + train_name)\n",
        "                !mv {date_path+filename} {gen_directory+train_name}\n",
        "                file_cnt+=1\n",
        "            elif filename[:5]=='test_':\n",
        "                test_name= 'test_gen.jsonl'\n",
        "                print('Renaming '+ filename+ ' to ' + test_name)\n",
        "                !mv {date_path+filename} {gen_directory+test_name}\n",
        "                file_cnt+=1\n",
        "    if file_cnt < 2:\n",
        "        print(\"ERROR: train_gen.jsonl and/or test.jsonl not created\")\n",
        "    elif os.path.getsize(gen_directory+train_name) == 0:\n",
        "        print(\"ERROR: train_gen.jsonl file is empty\")\n",
        "    elif os.path.getsize(gen_directory+test_name) == 0:\n",
        "        print(\"ERROR: test_gen.jsonl file is empty\")\n",
        "    else:\n",
        "        print(\"Training and test files successfully created in: \" + gen_directory)\n",
        "except:\n",
        "    print(\"Error running ilab generate, no synthetic data generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0B_39Sqvxgj"
      },
      "source": [
        "### 2.5 Show examples of generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLqnVW7rvxgj"
      },
      "outputs": [],
      "source": [
        "print(\"2.4.3 Show examples of generated data\")\n",
        "for filename in os.listdir(gen_directory):\n",
        "    if filename[:9]=='train_gen':\n",
        "        with open(gen_directory+filename, 'r') as syn_file:\n",
        "            cnt=0\n",
        "            for line_number, line in enumerate(syn_file):\n",
        "                if cnt >= 8:\n",
        "                    break\n",
        "                jsonLine= json.loads(line)\n",
        "                syn_user=jsonLine[\"user\"]\n",
        "                syn_assist=jsonLine[\"assistant\"]\n",
        "                #Remove \"Answer:\" and \"Response:\" from answers for displaying\n",
        "                if syn_user[:10]==\"Question: \":\n",
        "                    syn_user=syn_user[10:]\n",
        "                if syn_assist[:8]==\"Answer: \":\n",
        "                    syn_assist=syn_assist[8:]\n",
        "                cnt+=1\n",
        "                print(\"\\nQuestion: \"+syn_user+\"\\nAnswer: \"+syn_assist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvTYe8oHdm13",
        "tags": []
      },
      "source": [
        "## Section 3. Training with InstructLab\n",
        "\n",
        "### 3.1 Select the model training pipeline\n",
        "\n",
        "InstructLab has three primary model training pipelines: simple, full (default), and accelerated. For all of the models, the training time can be limited by adjusting the num_epoch paramater. The maximum number of epochs for running the InstructLab end-to-end workflow is 10.\n",
        "\n",
        "#### **Simple pipeline**\n",
        "\n",
        "The simple pipeline uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process. The simple pipeline only works with Merlinite 7b Lab as the teacher model. For this Linux system, the trained model is saved in the models directory as ggml-model-f16.gguf.\n",
        "\n",
        "The command form is:\n",
        "\n",
        "    ilab model train --pipeline simple\n",
        "\n",
        "**Note:** This process will take a little while to complete (time can vary based on hardware and output of ilab data generate but on the order of 5 to 15 minutes)\n",
        "\n",
        "#### **Accelerated pipeline**\n",
        "\n",
        "The accelerated uses the instructlab-training library which supports GPU accelerated and distributed training. The full loop and data processing functions are either pulled directly from or based off of the work in this library. For the accelerated pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one. Training is support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.\n",
        "\n",
        "The command form is:\n",
        "\n",
        "    ilab model train --pipeline accelerated --device cuda --data-path <path-to-sdg-data>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UrXh9-nvxgj"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Select to Continue or to Train the model\")\n",
        "display(train_pipe)\n",
        "display(epoch)\n",
        "display(it)\n",
        "print(\"After choosing your training options, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUPA7SpTvxgj"
      },
      "source": [
        "### 3.2 Run the model training\n",
        "\n",
        "Model training can take 30 minutes or more for 1 epoch and 1 iteration and takes 1 hour for the default paramter values. This minimal training could be used for testing the generation and training for a new set of data.\n",
        "\n",
        "To produce a higher quality model, more epochs and iterations are needed for refining the model. This will require a proportionally longer time to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcIjCQUjvxgj",
        "scrolled": true,
        "outputId": "74be7779-9f54-4625-834a-53c0628be375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create directory: data/oscars/new_model/\n",
            "Train with simple pipeline with a GPU\n",
            "Running: !ilab model train --pipeline simple --device cuda --model-path models/instructlab/granite-7b-lab --data-path data/oscars/ilab_generated/ --num-epochs 3 --iters 5\n",
            "INFO 2025-06-03 03:03:44,103 datasets:54: PyTorch version 2.5.1 available.\n",
            "INFO 2025-06-03 03:03:44,104 datasets:66: Polars version 1.21.0 available.\n",
            "INFO 2025-06-03 03:03:44,105 datasets:77: Duckdb version 1.2.2 available.\n",
            "INFO 2025-06-03 03:03:44,105 datasets:112: TensorFlow version 2.18.0 available.\n",
            "INFO 2025-06-03 03:03:44,106 datasets:125: JAX version 0.5.2 available.\n",
            "2025-06-03 03:03:48.884246: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-03 03:03:48.902077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748919828.923603   12447 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748919828.930176   12447 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-03 03:03:48.951897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LINUX_TRAIN.PY: NUM EPOCHS IS:  3\n",
            "LINUX_TRAIN.PY: TRAIN FILE IS:  data/oscars/ilab_generated/train_gen.jsonl\n",
            "LINUX_TRAIN.PY: TEST FILE IS:  data/oscars/ilab_generated/test_gen.jsonl\n",
            "LINUX_TRAIN.PY: Using device 'cuda:0'\n",
            "  NVidia CUDA version: 12.4\n",
            "  AMD ROCm HIP version: n/a\n",
            "  cuda:0 is 'NVIDIA A100-SXM4-40GB' (39.1 GiB of 39.6 GiB free, capability: 8.0)\n",
            "LINUX_TRAIN.PY: LOADING DATASETS\n",
            "Generating train split: 3356 examples [00:00, 315848.05 examples/s]\n",
            "Generating train split: 405 examples [00:00, 83560.09 examples/s]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
            "LINUX_TRAIN.PY: NOT USING 4-bit quantization\n",
            "LINUX_TRAIN.PY: LOADING THE BASE MODEL\n",
            "Loading checkpoint shards: 100% 3/3 [00:00<00:00, 44.89it/s]\n",
            "LINUX_TRAIN.PY: Model device cuda:0\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12852 MiB |  12852 MiB |  12852 MiB |      0 B   |\n",
            "|       from large pool |  12852 MiB |  12852 MiB |  12852 MiB |      0 B   |\n",
            "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12852 MiB |  12852 MiB |  12852 MiB |      0 B   |\n",
            "|       from large pool |  12852 MiB |  12852 MiB |  12852 MiB |      0 B   |\n",
            "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12852 MiB |  12852 MiB |  12852 MiB |      0 B   |\n",
            "|       from large pool |  12852 MiB |  12852 MiB |  12852 MiB |      0 B   |\n",
            "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  12862 MiB |  12862 MiB |  12862 MiB |      0 B   |\n",
            "|       from large pool |  12860 MiB |  12860 MiB |  12860 MiB |      0 B   |\n",
            "|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     292    |     292    |     292    |       0    |\n",
            "|       from large pool |     226    |     226    |     226    |       0    |\n",
            "|       from small pool |      66    |      66    |      66    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     292    |     292    |     292    |       0    |\n",
            "|       from large pool |     226    |     226    |     226    |       0    |\n",
            "|       from small pool |      66    |      66    |      66    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "LINUX_TRAIN.PY: SANITY CHECKING THE BASE MODEL\n",
            "  0% 0/405 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_logits` is. When `return_dict_in_generate` is not `True`, `output_logits` is ignored.\n",
            "  warnings.warn(\n",
            " 24% 96/405 [05:43<13:15,  2.58s/it]"
          ]
        }
      ],
      "source": [
        "data_path=\"data/\"+ use_case+\"/ilab_generated/\"\n",
        "train_data=data_path+\"train_gen.jsonl\"\n",
        "model_path=\"models/instructlab/granite-7b-lab\"\n",
        "##model_path='/root/.cache/instructlab/models/instructlab/granite-7b-lab'\n",
        "trained_model_path=\"data/\"+ use_case+\"/new_model/\"\n",
        "\n",
        "##'Simple (Fast)', 'Accelerated GPU'\n",
        "file_cnt=0\n",
        "for filename in os.listdir(data_path):\n",
        "    if filename[:15]=='train_gen.jsonl': file_cnt+=1\n",
        "    elif filename[:14]=='test_gen.jsonl': file_cnt+=1\n",
        "if file_cnt < 2 or os.path.getsize(gen_directory+train_name) < 5 or os.path.getsize(gen_directory+test_name) < 5:\n",
        "    print(\"ERROR: train_gen.jsonl and/or test.jsonl are not present or too small\")\n",
        "\n",
        "if not os.path.exists(trained_model_path):\n",
        "    print(\"Create directory: \" + trained_model_path)\n",
        "    !mkdir {trained_model_path}\n",
        "ep=int(epoch.value)\n",
        "its=int(it.value)\n",
        "if train_pipe.value=='Simple with GPU':\n",
        "    print(\"Train with simple pipeline with a GPU\")\n",
        "    shell_command = f\"ilab model train --pipeline simple --device cuda --model-path {model_path} --data-path {data_path} --num-epochs {ep} --iters {its}\"\n",
        "elif train_pipe.value=='Accelerated GPU':\n",
        "    print(\"Train accelerated with a GPU\")\n",
        "    #shell_command = f\"ilab model train --pipeline accelerated --device cuda --model-path {model_path} --data-path {train_data} --num-epochs {ep} --iters {its}\"\n",
        "    shell_command = f\"ilab -v -v model train --pipeline accelerated --device cuda --model-path {'/content/ilab/'+model_path} --data-path {'/content/ilab/'+train_data}\"\n",
        "\n",
        "print(\"Running: !\"+shell_command)\n",
        "!{shell_command}\n",
        "if train_pipe.value=='Accelerated GPU':\n",
        "    print(\"Run ilab model evaluate\")\n",
        "    !ilab model evaluate --benchmark mmlu\n",
        "#Move the model to the use_case/new_model directory\n",
        "print(\"Moving the trained model to the directory: \"+trained_model_path)\n",
        "!mv /root/.local/share/instructlab/checkpoints/ggml-model-f16.gguf {trained_model_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SogwbM1vxgk",
        "tags": []
      },
      "source": [
        "# Section 4. Inferencing with the Model\n",
        "\n",
        "You have now completed InstructLab training. You can run this section to ask questions to both the base and InstructLab trained models and to compare answers.\n",
        "\n",
        "This third notebook section showcases the generation of synthetic data utilizing InstructLab. It subsequently demonstrates how a large language model (LLM) can be effectively trained on this synthetic dataset. In current notebook, Both the pre-trained LLM and the LLM trained on the generated synthetic data are evaluated against a predefined set of questions to assess their respective performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWNj4u9Vvxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_3\"></a>\n",
        "## Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance\n",
        "\n",
        "Run both base and trained models to compare results with interactive questions and and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoD1Ngrv7ni3"
      },
      "outputs": [],
      "source": [
        "print(\"Do you want to run interactive question on the base and trained models?\")\n",
        "display(questions)\n",
        "print(\"After making your choice, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK5l-lIE7oPF"
      },
      "source": [
        "The following are sample questions derived from the data used to generate synthetic data, which was then employed to train the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVp-BeByvxgl"
      },
      "outputs": [],
      "source": [
        "# Define a Function to Perform Inference on Base and Trained Models\n",
        "def model_inference(base_model_path, trained_model_path):\n",
        "    _DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "    Current conversation:\n",
        "    Human: {input}\n",
        "    AI:\"\"\"\n",
        "    base_llm = LlamaCpp(model_path=base_model_path,\n",
        "                   verbose=False,\n",
        "                   n_gpu_layers=25,\n",
        "                   max_tokens=90,\n",
        "                   temperature=0,\n",
        "                   top_k=1\n",
        "                  )\n",
        "    trained_llm = LlamaCpp(model_path=trained_model_path,\n",
        "                   verbose=False,\n",
        "                   n_gpu_layers=25,\n",
        "                   max_tokens=90,\n",
        "                   temperature=0,\n",
        "                   top_k=1\n",
        "                  )\n",
        "    PROMPT = PromptTemplate( input_variables=[\"input\"],\n",
        "                            template=_DEFAULT_TEMPLATE\n",
        "                            )\n",
        "    chain1 = PROMPT | base_llm | StrOutputParser()\n",
        "    chain2 = PROMPT | trained_llm | StrOutputParser()\n",
        "    while True:\n",
        "        question = input(\"Ask me a question (type 'exit' to end): \")\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting this Q&A session.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"You asked: \", question)\n",
        "            answer1 = chain1.invoke(question)\n",
        "            answer1= answer1.split('Human',1)[0]\n",
        "            print (\"Base Model Answer: \",answer1)\n",
        "            answer2 = chain2.invoke(question)\n",
        "            answer2= answer2.split('Human',1)[0]\n",
        "            print (\"Trained Model Answer: \",answer2)\n",
        "\n",
        "##Display Sample Questions\n",
        "base_model = notebook_dir +\"/models/granite-7b-lab-Q4_K_M.gguf\"\n",
        "trained_model = trained_model_path + \"ggml-model-f16.gguf\"\n",
        "if questions.value=='Yes':\n",
        "  with open(notebook_dir+'/data/' + use_case + '/questions.txt') as f:\n",
        "      for line in f.readlines():\n",
        "          display(widgets.HTML(Norm+line))\n",
        "  print(\"Processing may take several minutes on the first run...\")\n",
        "  model_inference(base_model, trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5l5m7L_LDV1"
      },
      "source": [
        "# Section 5. Download the Trained Model\n",
        " Now that we have a model trained on our dataset, we can download the trained model for futher testing and use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx65h6u17SW_"
      },
      "outputs": [],
      "source": [
        "print(\"Do you want to download the trained model to your local machine?\")\n",
        "display(download)\n",
        "print(\"After making your selection, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwwZ4P6n7TDu"
      },
      "source": [
        "Select and run the next cell to download if selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZsjOChUK7gj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "if download.value=='Yes':\n",
        "  files.download(trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K4lhpzTvxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_conclusion\"></a>\n",
        "# Conclusion\n",
        "\n",
        "This notebook demonstrated utilizing InstructLab for introducing datasets, data generation, model training, and model creation. This notebook produced an InstructLab trained model that was available for inferecing and downloading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63hBXXa0vxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_learn\"></a>\n",
        "# Learn More\n",
        "\n",
        "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
        "\n",
        "This notebook is based on the InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
        "\n",
        "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poB7nDmcvxgl"
      },
      "source": [
        " 2025 IBM Corporation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}